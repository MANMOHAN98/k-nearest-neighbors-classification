
Implement and understand the working of the K-Nearest Neighbors (KNN) algorithm for classification using Python and scikit-learn.



 What I Learned
- Instance-based learning approach
- Euclidean distance calculation
- Importance of feature normalization
- How to choose the value of *K*
- Visualizing decision boundaries
- Evaluating performance using accuracy and confusion matrix

 Tools & Libraries Used
- Python
- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn

 Dataset
- *Iris Dataset* (from [UCI ML Repository](https://www.kaggle.com/datasets/uciml/iris))

 Steps Followed
1. *Data Loading*: Loaded the Iris dataset.
2. *Preprocessing*: Normalized the features for fair distance calculation.
3. *Train-Test Split*: Divided the dataset into training and testing sets.
4. *Model Building*: Used KNeighborsClassifier from sklearn.
5. *Hyperparameter Tuning*: Tested various values of K.
6. *Evaluation*:
   - Accuracy Score
   - Confusion Matrix
7. *Visualization*:
   - Accuracy vs. K plot
   - Confusion matrix plot

 Results
- Best K value was selected based on accuracy.
- Final model achieved high accuracy.
- Visualization of confusion matrix was done for clarity.



